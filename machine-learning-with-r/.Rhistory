fit(outcome ~ .,
data = mutagen_juiced) -> rf_mutagen_final_fit
tictoc::toc(log = TRUE)
## write to disk
write_rds(
rf_mutagen_final_fit,
here::here(
"data",
"rf_mutagen_final_fit.rds"
)
## read from disk
rf_mutagen_final_fit <- read_rds(
here::here(
"data",
"rf_mutagen_final_fit.rds")
)
rf_mutagen_final_fit %>%
vip(geom = "point")
# Chunk 35
rf_final_wf <- workflow() %>%
add_recipe(rf_rec) %>%
add_model(final_rf)
rf_final_res <- rf_final_wf %>%
last_fit(mutagen_split)
rf_final_res %>%
collect_metrics()
# Chunk 36
rf_auc <-
rf_final_res %>%
collect_predictions() %>%
roc_curve(outcome, .pred_mutagen) %>%
mutate(model = "Random Forest")
lr_auc <- lr_testing_pred %>%
roc_curve(truth = outcome, .pred_mutagen) %>%
mutate(model = "Logistic Regression")
## compare logistic regression and RF
bind_rows(rf_auc, lr_auc) %>%
ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) +
geom_path(lwd = 1.5, alpha = 0.8) +
geom_abline(lty = 3) +
coord_equal() +
scale_color_viridis_d(option = "plasma", end = .6)
# Chunk 37
knitr::include_graphics(
here::here(
"images",
"conf_mat_spec_sele.png"
)
# Chunk 38
rf_final_res %>%
collect_predictions() -> predictions_tbl
conf_mat(predictions_tbl, truth = outcome, estimate = .pred_class)
# Chunk 39
## Inspect the data
dim(data_tame)
data_tame[1:4,1:5]
# or
data_tame
unique(data_tame$List)
colnames(data_tame)
# Chunk 40
## exploratory graphs
data_tame_tidy <- data_tame |>
rename(class = List) |>
mutate(class = as_factor(class)) |>
janitor::clean_names()
names(data_tame_tidy)
data_tame_tidy |>
ggplot(aes(x = molecular_weight, y = opera_octanol_water_distribution_coefficient)) +
geom_point(aes(colour = class))
## let's do another
data_tame_tidy |>
ggplot(aes(x = opera_water_solubility , y = opera_negative_log_of_acid_dissociation_constant)) +
geom_point(aes(colour = class))
## What can you conclude from these two example graphs?
## missing values
sum(is.na(data_tame_tidy))
library(naniar)
vis_miss(data_tame_tidy)
## missingnes seems to be in the dtxsid colum
## check
sum(is.na(data_tame_tidy$dtxsid))
# > 47, so not a problem, this variable is an ID, not a predictor. For random forest, we cannot have missing values in predictor variables
# Chunk 41
## define the engine and model type
## this problem seems fit for a regression tree approach, e.g. random forest. See: https://parsnip.tidymodels.org/reference/rand_forest.html
tame_mod <- rand_forest() |>
set_engine("ranger") |>
set_mode("classification")# default engine
tame_mod
# Chunk 42
## data split
data_tame_split <- initial_split(data_tame_tidy, prop = 3/4)
# Create data frames for the two sets:
data_tame_train <- training(data_tame_split)
data_tame_test  <- testing(data_tame_split)
# Chunk 43
names(data_tame_tidy)
## define the recipe
tame_rec <- recipe(class ~ ., data = data_tame_train) |>
update_role(substance_name, casrn, dtxsid, new_role = "ID") %>%
step_normalize(all_numeric_predictors()) |>
step_center(all_numeric_predictors()) |>
step_zv(all_predictors())
## let's look at the distribution of values accross the predictors
map_lgl(
data_tame_tidy,
is.numeric
) -> ind
map_df(
data_tame_tidy[,ind],
min
) -> mins
mins$type = "min"
map_df(
data_tame_tidy[,ind],
max
) -> maxes
maxes$type = "max"
bind_rows(mins, maxes) |>
pivot_longer(1:10, names_to = "vars", values_to = "values") |>
ggplot(aes(
x = reorder(as_factor(vars), values),
y = values)) +
geom_point(aes(colour = type), position = "jitter") +
toolboxr::rotate_axis_labels("x", 90)
## now we do the same for the normalized and centered data, we will use the function `preProcess()` from the {caret} package, which was developed by Max Kuhn (co/lead-developer for {tidymodels})
library(caret)
preProcValues <- preProcess(data_tame, method = c("center", "scale"))
data_tame_transformed <- predict(preProcValues, data_tame)
data_tame_transformed
## let's put the code above in a function that we can recycle for later use
plot_min_max <- function(df, ...) { ## ... ellipsis, additional arguments to pivot_longer(), column index or tidy-eval column names (unquoted)
map_lgl(df,
is.numeric) -> ind
map_df(df[, ind],
min) -> mins
mins$type = "min"
map_df(df[, ind],
max) -> maxes
maxes$type = "max"
title <- deparse(substitute(df))
bind_rows(mins, maxes) |>
pivot_longer(..., names_to = "vars", values_to = "values") |>
ggplot(aes(x = reorder(as_factor(vars), values),
y = values)) +
geom_point(aes(colour = type), position = "jitter") +
ggtitle(title) +
toolboxr::rotate_axis_labels("x", 90) -> p
return(p)
}
plot_min_max(df = data_tame_transformed, 1:10) -> tame_plot_transformed
plot_min_max(df = data_tame, 1:10) -> tame_plot
cowplot::plot_grid(
tame_plot,
tame_plot_transformed
)
# Chunk 44
## In terms of feature engineering, there seems to be no need to do this upfront. We could get the SMILES for each compound from a database, and calculate fingerprints. In a later part of this lesson we will revisit this option.
# Chunk 45
tame_workflow <- workflow() |>
add_model(tame_mod) |>
add_recipe(tame_rec)
# Chunk 46
## fitting the model
tame_fit <- tame_workflow |>
fit(data = data_tame_train)
predict(tame_fit, data_tame_test)
predict(tame_fit, data_tame_test, type = "prob")
## augment
tame_aug <-
augment(tame_fit, data_tame_test, .pred_PFAS, .pred_Statins)
## The data look like:
tame_aug %>%
select(class, .pred_class, .pred_PFAS, .pred_Statins)
## Confusion table
tame_aug %>%
conf_mat(truth = class, estimate = .pred_class)
## Accuracy
# Model preformace metrics
class_metrics <- metric_set(accuracy)
## Get preformance metrics
tame_aug %>%
class_metrics(truth = class, estimate = .pred_class)
# Chunk 47
tame_importance_mod <- decision_tree() |>
set_engine("rpart") |>
set_mode("classification")# default engine
tame_rpart_wf <- tame_workflow |>
update_model(tame_importance_mod)
## new fit using the updated workflow
tame_importance_fit <- tame_rpart_wf |>
fit(data = data_tame_train)
## decision tree
library(rpart.plot)
tame_importance_fit %>%
extract_fit_engine() %>%
rpart.plot(roundint = FALSE)
# Chunk 48
list.files(here::here("data"), full.names = TRUE, pattern = ".csv")
# Chunk 49
data_qsar <- read_csv2(
here::here(
"data-raw",
"qsar_oral_toxicity.csv"),
col_names = FALSE) |>
relocate()
# Chunk 50
# answer
names_new <- paste0("f", 1:1025)
names(data_qsar) <- names_new
data_qsar <- data_qsar |>
dplyr::relocate(f1025, .before = f1) |>
rename(class = f1025)
data_qsar |>
group_by(class) |>
tally()
classes <- data_qsar$class
#data_qsar_all_numm <- data_qsar |>
#  select(-class)
## look at the data
data_qsar
# Chunk 51
sum(is.na(data_qsar))
# Chunk 52
data_qsar_mtx <- data_qsar |>
select(-class) |>
as.matrix()
features <- colSums(data_qsar_mtx) |>
enframe()
features |>
ggplot(aes(x = value)) +
geom_histogram()
# Chunk 53
features_negative <- data_qsar |>
dplyr::filter(class == "negative") |>
select(-class) |>
as.matrix() |>
colSums() |>
enframe() |>
mutate(distro = "negative")
features_positive <- data_qsar |>
dplyr::filter(class == "positive") |>
select(-class) |>
as.matrix() |>
colSums() |>
enframe() |>
mutate(distro = "positive")
features_neg_pos <- dplyr::bind_rows(
features_negative,
features_positive
)
features_neg_pos |>
ggplot(aes(x = value)) +
geom_freqpoly(aes(colour = distro), alpha = 0.8)
# Chunk 54
data_qsar |>
group_by(class) |>
tally() -> tally_compounds
features_row_neg <- data_qsar |>
dplyr::filter(class == "negative") |>
select(-class) |>
as.matrix() |>
rowSums() |>
enframe() |>
mutate(distro = "negative")
features_row_pos <- data_qsar |>
dplyr::filter(class == "positive") |>
select(-class) |>
as.matrix() |>
rowSums() |>
enframe() |>
mutate(distro = "positive")
features_row_neg_pos <- dplyr::bind_rows(
features_row_neg,
features_row_pos
)
features_row_neg_pos |>
ggplot(aes(x = value)) +
geom_freqpoly(aes(colour = distro), alpha = 0.8)
features_row_neg_pos |>
group_by(distro) |>
summarise(mean_feat = mean(value))
# Chunk 57
library(tidymodels)
library(tidyverse)
#library(gapminder)
theme_set(theme_bw(16))
# Chunk 59
#install.packages("embed")
library(embed)
pca_recipe <- recipe(~., data = data_qsar)
pca_trans <- pca_recipe %>%
# center the data
step_center(all_numeric()) %>%
# center the data
step_scale(all_numeric()) %>%
# pca on all numeric variables
step_umap(all_numeric())
pca_estimates <- prep(pca_trans) ## this step takes a bit longer to calculate
pca_estimates$var_info
juice(pca_estimates)
juice(pca_estimates) %>%
ggplot(aes(UMAP1, UMAP2)) +
geom_point(aes(color = class), alpha = 0.3, size = 2)+
labs(title="UMAP (t-SNE) from tidymodels")
# Chunk 61
## prepare data splits
set.seed(123)
qsar_split <- initial_split(data = data_qsar,
prop = 0.80,
strata = class)
qsar_train <- training(qsar_split)
qsar_test <- testing(qsar_split)
# Chunk 62
qsar_test |>
group_by(class) |>
tally()
qsar_train |>
group_by(class) |>
tally()
qsar_test |>
group_by(class) |>
tally()
qsar_test |>
group_by(class) |>
tally()
qsar_train |>
group_by(class) |>
tally()
## prepare model recipe
xgb_mod <- boost_tree(mtry = 50, trees = 500) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_rec <- recipe(class ~ ., data = qsar_train) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors()) |>
step_zv(all_predictors())
xgb_wf <- workflow() %>%
add_model(xgb_mod) %>%
add_recipe(xgb_rec)
data_qsar <- data_qsar |>
mutate(class = as_factor(class))
qsar_test <- qsar_test |>
mutate(class = as_factor(class))
qsar_train <- qsar_train |>
mutate(class = as_factor(class))
set.seed(1)
## fit model
xgb_fit <- xgb_wf %>%
fit(data = qsar_train)
xgb_fit %>% extract_fit_parsnip()
predict(xgb_fit, qsar_test)
## prepare model recipe
xgb_mod <- boost_tree(mtry = 50, trees = 500) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_rec <- recipe(class ~ ., data = qsar_train) |>
step_center(all_numeric_predictors()) |>
step_scale(all_numeric_predictors()) |>
step_zv(all_predictors())
xgb_wf <- workflow() %>%
add_model(xgb_mod) %>%
add_recipe(xgb_rec)
#prep <- prep(rf_rec)
#juiced <- juice(prep)
data_qsar <- data_qsar |>
mutate(class = as_factor(class))
qsar_test <- qsar_test |>
mutate(class = as_factor(class))
qsar_train <- qsar_train |>
mutate(class = as_factor(class))
set.seed(1)
## fit model
xgb_fit <- xgb_wf %>%
fit(data = qsar_train)
## see model metrics
xgb_fit %>% extract_fit_parsnip()
predict(xgb_fit, qsar_test)
## Model eval
xgb_fit %>%
predict( new_data = qsar_test) %>%
bind_cols(qsar_test["class"]) %>%
accuracy(truth= as.factor(class), .pred_class)
## Model eval
xgb_fit %>%
predict( new_data = qsar_test) %>%
bind_cols(qsar_test["class"]) %>%
accuracy(truth= class, .pred_class)
## confusion matrix
caret::confusionMatrix(
as.factor(qsar_test$class),
predict(xgb_fit, new_data = qsar_test)$.pred_class)
bind_cols(
predict(xgb_fit, qsar_test),
predict(xgb_fit, qsar_test, type = "prob"),
qsar_test[,1]
) -> predictions
predictions
library(tidymodels)  # for the tune package, along with the rest of tidymodels
# Helper packages
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
xgb_mod_tune <- boost_tree(
mtry = tune(),
trees = tune(),
tree_depth = tune()) %>%
set_engine("xgboost") %>%
set_mode("classification")
tree_grid <- grid_regular(
trees(),
tree_depth(),
finalize(mtry(), select(data_qsar , -class)),
levels = 3)
tree_grid
set.seed(234)
cell_folds <- vfold_cv(qsar_train, v = 3)
source("~/Documents/workspaces/course-materials/track-a/machine-learning-with-r/render_book.R")
bookdown::publish_book()
usethis::use_package(c("parsnip", "tune", "yardstick", "recipe", "dplyr", "here", "tidyr", "ggplot2", "janitor"))
pkgs <- c("parsnip", "tune", "yardstick", "recipe", "dplyr", "here", "tidyr", "ggplot2", "janitor")
map(pkgs, usethis::use_package)
pkgs <- c("parsnip", "tune", "yardstick", "dplyr", "here", "tidyr", "ggplot2", "janitor")
map(pkgs, usethis::use_package)
usethis::use_package("recipes")
library(caait)
citation("caait")
library(caait)
citation("caait")
library(caait)
citation("caait")
usethis::use_ccby_license()
list.files(here::here("data-raw"), full.names = TRUE, pattern = ".csv")
data_qsar <- read_csv2(
here::here(
"data-raw",
"qsar_oral_toxicity.csv"),
col_names = FALSE)
library(tidyverse)
data_qsar <- read_csv2(
here::here(
"data-raw",
"qsar_oral_toxicity.csv"),
col_names = FALSE)
View(data_qsar)
data_qsar <- read_csv2(
here::here(
"data-raw",
"qsar_oral_toxicity.csv"),
col_names = FALSE) |>
relocate(X1025)
View(data_qsar)
data_qsar <- read_csv2(
here::here(
"data-raw",
"qsar_oral_toxicity.csv"),
col_names = FALSE)
# answer
names_new <- paste0("f", 1:1025)
names(data_qsar) <- names_new
data_qsar <- data_qsar |>
dplyr::relocate(f1025, .before = f1) |>
rename(class = f1025)
data_qsar |>
group_by(class) |>
tally()
classes <- data_qsar$class
#data_qsar_all_numm <- data_qsar |>
#  select(-class)
## look at the data
data_qsar
RStudio.Version()
tools::RStudio.Version()
?RStudio.Version
version
load(
here::here(
"data-raw",
"mutagen_tbl.Rda"
)
library(tidyverse)
library(tidymodels)
library(skimr)
mutagen_tbl |>
glimpse()
mutagen_tbl[ , c(1:5)] |>
glimpse()
pak::pkg_install("tidyverse")
pak::pkg_install(c("skimr", )
pak::pkg_install(c("skimr", "lme4")
)
source(
"https://github.com/eurotox-2023-aitox-cec/course-materials/blob/main/dependencies-r.R"
)
download.file("https://github.com/eurotox-2023-aitox-cec/course-materials/blob/main/dependencies-r.R")
download.file("https://github.com/eurotox-2023-aitox-cec/course-materials/blob/main/dependencies-r.R", destfile = "temp.R")
source("temp.R")
system("curl -O https://github.com/eurotox-2023-aitox-cec/course-materials/blob/main/dependencies-r.R")
source("https://raw.githubusercontent.com/eurotox-2023-aitox-cec/course-materials/main/dependencies-r.R")
install.packages("pak")
12*120
pak::pkg_install("openair")
2288*0.2
300*12
2288*0.1
2288*0.1 -> x
x/3
(x/3) I *2
(x/3)*2
2288*0.1 -> x
x
12*((x/3)*2)
